ex2 -

	The sigmoid function can give you the probability of an event.

	prob = sigmoid([1 45 85] * theta);

	What is the probability a student with exam scores 45 and 85 will get in?

	------------------------
	For regularized logistic regression, it uses the same equation as logist regression except an additional sum.
	When we calculate the cost and gradient descent, we ignore theta 0. 
	For the cost calculation, when we sum up the theta squares, we do not include theta 0.
	For the gradient descent, all features, except the first one, will we calculate the gradient descent. The gradient descent for the first theta is just the logistic gradient descent calculation.
	------------------------
	Lambda:
	A large lambda value would make the thetas count less towards the model. Too large lambda, meaning we want the thetas to have less effect, you end up with only theta0. Underfitting.
	A small lambda, you get a really good fit with the data. This would result in overfitting the data. The equation is almost perfect, which is unreasonable.

ex3 - 

    We start with the one-vs-all classifier. 
	    First, with the existing data set we train by calculate the best theta. For each class label (in this case it's numbers 1 to K=10), we apply regularized logistic regression to the data set. fmincg is used to find the minimum cost of the function (in this case the regularized logistic function) and with it we can find the best thetas used for each class label. The best thetas are found by finding the lowest cost of the function. The lowest cost really tells you how well the equation fits the data.
	    With the thetas, we can calculate h(x), which gives you the probability of an event (for the sigmoid function, see ex2).

	    So for each class label, we have the "best" theta values. 
	    For each training sample (each row), we need to calculate h(x) by multiplying the training sample by the theta values for each class.
	    So we are getting the probability (h(c)) for each training sample for each class label. The matrix multiplication will work this out and the max function will tell you the column index with the max value. The max column index will be the class label with the best probability.

    	Overall summary-
    	We trained our thetas for each class using the existing data. With the thetas for each class, we can determine the probability of a sample belonging to a certain class. We are basically creating an equation that best fits the data and applying that equation to a sample.

    predict.m is interesting because we apply logistic regression twice. The idea is similar to one-vs-all classifier, where we have the "best" theta values and we apply that to the data X. For each row sample x, we multiply it by the thetas for each class, which give us the probability of the chance of that class being true (applying sigmoid too). Once we do that, we get a matrix that looks similar to X, except each column represents h(x) for a class and each row represents each row in X, which is just each x value for a given row sample (x0,x1,x2...). So we then get a matrix that is dimensions num samples in X by num classes. What is interesting is that when we apply logistic regression to it a second time, the x values now become h(x) values. The number of rows never actually change... At the end, we then figure out which class has the max probability, and that becomes our prediction.


 ex4 -

 	For the cost function, since yk(i) is a vector such as [0 0 0 1 .. 0 0], where 1 is the true value of the given sample, we don't need to do the entire multiplication for the first part (-yk(i) * log((h(x(i))k)). The reason is because those y values are zeros. We just need to do the multiplication for the yth label. 
 	For regularization, we need to make sure not to include the bias unit (the first column) for each theta

 	Backpropagation -
 	Random initialization of weights for theta - 
 		A good way to initialize epsilon is to set it to = (root 6) / (root (L_in + L_out)), where L_in = number of units in current layer and L_out is the number of units in the adjacent layer

 	The error term is calculated for layers 2:L. i and j are the units for layer l and layer l + 1 respectively. g'(z_2) is also equal to a_2 * (1 - a_2). 
 	For regularization, we skip the bias unit or the first column of theta.
 	We play with lambda and num iterations values (For calculating thetas).
 	A small lambda value seems to correlate with better fit. The same for increasing the number of iterations.
 	Smaller lambda means you would get a really good fit with the data, so that makes sense.
 	Increasing the number of iterations, which decreases the cost (as seen on console), improves our accuracy also. This makes sense because we would getting better theta values because the cost is decreasing.

 	
Bias vs Variance
	High bias means that the thetas are underfitting the training data.
		J_train(theta) will be high and J_train(theta) ~ J_cv(theta)
	High variance means that the thetas are overfitting the training data.
		J_train(theta) will be low, but J_cv(theta) >> J_train(theta)

	Increasing the degree d of the polynomial will make the training error go down because we get a better fit. However, the cross validation error will tend to decrease up to a certain point, and then increase... forming a convex curve.

	Getting more training examples will help with high variance, but not high bias.

	The test error and train error curves are similar to high bias vs high variance. 
	Test error refers to J_cv(theta) actually
	High bias -
		Low training set size: J_train(theta) is low and J_cv(theta) is high
		High training data size: J_train(theta) and J_cv(theta) are high with J_train(theta) ~= J_cv(theta)
	High variance -
		Low training data size: J_train(theta) is low and J_vc(theta) is high
		High training data size: J_train(theta) increases and J_cv(theta) decreases and levels off. J_train(theta) is < J_cv(theta), but the difference between them remains significant.

	Cases -
	1. Getting more training examples: Helps with fixing high variance because the error curve for the cross validation data set decreases when we get more examples. Whereas the error curve for the training data increases. We want to decrease the cross validation error because it helps us train new examples better.
	2. Getting smaller sets of features: Decreasing the number of thetas. Aka, making the data fit less. Addresses overfitting or high variance.
	3. Getting more features: So we are better fitting the data by getting more features. This would help high bias because we want to get a better fit.
	4. Adding polynomial features: Same idea as above. More polynomial features will result in better fit. High bias.
	5. Decreasing lambda: Decreasing lambda for regularization would make the thetas have more of an impact. This would result in fitting the data better. So it addresses high bias, underfitting.
	6. Increasing lamda: Increasing lambda for regularization would make the thetas have less of an impact. This would result in worst fitting of the data. So it addresses high variance.

	Improve high bias-
		3. Getting more features
		4. Adding polynomial features
		5. Decreasing lambda
	Improve high variance
		1. Getting more training examples
		2. Getting smaller sets of features
		6. Increasing lamda



	Neural network with fewer parameters vs more parameters.
	Fewers Parameters -
		The model is more prone to underfitting.
		Computationally cheaper
	More parameters.
		The model is more prone to overfitting. 
		Computationally more expensive.
		We can mitigate overfitting by increasing lambda (for regularization).

ex5 - 
	The training error and validation error are calculated using the linear regression function. when we select lambda for cross validation, the lambda is only used for calculating the cost and the thetas values. The training error and validation error do not use the lambda value.